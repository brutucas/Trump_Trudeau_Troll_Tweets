{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet classification: Trump vs. Trudeau\n",
    "\n",
    "For this data science project, I aimed to classify tweets from two prominent and polarising North American politicians: Donald Trump and Justin Trudeau. \n",
    "\n",
    "[Donald Trump and Justin Trudeau shaking hands.](https://upload.wikimedia.org/wikipedia/commons/4/47/President_Donald_Trump_and_Prime_Minister_Justin_Trudeau_Joint_Press_Conference%2C_February_13%2C_2017.jpg)\n",
    "\n",
    "The task involved delving into the realm of social media text classification, and focusing specifically on the challenges posed by tweets, such as their brevity, tweet-specific syntax (e.g. mentions, hashtags, emoji, links, and usernames), and the need to develop effective strategies to process this text and make it legible to an ML model and useful for training purposes.\n",
    "\n",
    "### Data Collection and Preparation\n",
    "\n",
    "To start the project, I gathered tweets from both Donald Trump and Justin Trudeau. I utilized various data collection techniques and libraries to obtain a diverse and representative sample of tweets from each politician."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "Once the data was collected, I performed extensive preprocessing to clean and prepare the text data for analysis. This included removing irrelevant information such as links, emoji, and special characters, as well as standardizing text format and handling platform-specific conventions like mentions and hashtags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection\n",
    "\n",
    "For tweet classification, I experimented with several classification models, including Multinomial Naive Bayes, Linear Support Vector Classifier (SVC), and Passive Aggressive Classifier. Each model was evaluated based on its performance metrics and ability to accurately classify tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "To represent the text data in a format suitable for machine learning models, I employed various vectorization techniques such as CountVectorizer and TfidfVectorizer. These methods transformed the text data into numerical features while preserving important information about word frequency and importance.\n",
    "\n",
    "### Model Evaluation and Optimization\n",
    "\n",
    "To assess the performance of each classification model, I utilized metrics such as accuracy, precision, recall, and F1-score. Additionally, I employed techniques like cross-validation and grid search to optimize model parameters and improve classification performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "import random; random.seed(53)\n",
    "\n",
    "# Import all we need from sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the Collected Data\n",
    "\n",
    "Initially, I worked with a corpus of tweets collected in November 2017, stored in a CSV file. Utilizing a Pandas DataFrame, I imported this data to facilitate its preparation for machine learning with scikit-learn.\n",
    "\n",
    "Since the dataset was fetched via the Twitter API without any pre-existing split into training and testing sets, I needed to perform this division myself. I used the `train_test_split()` function with `random_state=53` and a test size of `0.33`, which ensured consistent results across different executions. \n",
    "\n",
    "This approach also guaranteed a sufficient amount of test data for the evaluation phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "tweet_df = pd.read_csv('datasets/trump_trudeau/trump_trudeau_tweets.csv')\n",
    "\n",
    "# Create target\n",
    "y = tweet_df['author']\n",
    "\n",
    "# Split training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    tweet_df['status'], y, \n",
    "    test_size=0.33, \n",
    "    random_state=53)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing the Tweets\n",
    "\n",
    "With the training and testing data prepared, the next step was to convert the tweets into a format suitable for machine learning. I used CountVectorizer and TfidfVectorizer to create vectorized representations of the tweets. \n",
    "\n",
    "First, I fitted these vectorizers to the data. Once the tweets were vectorized, I was ready to move forward with modeling using these new vector representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize count vectorizer\n",
    "count_vectorizer = CountVectorizer(stop_words='english', min_df=0.05, max_df=0.9)\n",
    "\n",
    "# Create count train and test variables\n",
    "count_train = count_vectorizer.fit_transform(X_train)\n",
    "count_test = count_vectorizer.transform(X_test)\n",
    "\n",
    "# Initialize tfidf vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', min_df=0.05, max_df=0.9)\n",
    "\n",
    "# Create tfidf train and test variables\n",
    "tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
    "tfidf_test = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Multinomial Naive Bayes Model\n",
    "\n",
    "After vectorizing the data, I trained my first model using the Multinomial Naive Bayes approach with both CountVectorizer and TfidfVectorizer data. My goal was to determine which vectorization method would enhance the model's performance and why.\n",
    "\n",
    "To evaluate the effectiveness of each model, I compared the accuracy scores from the test sets for both the CountVectorizer and TfidfVectorizer implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a MulitnomialNB model\n",
    "tfidf_nb = MultinomialNB()\n",
    "\n",
    "tfidf_nb.fit(tfidf_train, y_train)\n",
    "\n",
    "tfidf_nb_pred = tfidf_nb.predict(tfidf_test)\n",
    "\n",
    "# Calculate the accuracy of your predictions\n",
    "tfidf_nb_score = metrics.accuracy_score(y_test, tfidf_nb_pred)\n",
    "\n",
    "# Create a MulitnomialNB model\n",
    "count_nb = MultinomialNB()\n",
    "\n",
    "count_nb.fit(count_train, y_train)\n",
    "\n",
    "# Run predict on your count test data to get your predictions\n",
    "count_nb_pred = count_nb.predict(count_test)\n",
    "\n",
    "# Calculate the accuracy of your predictions\n",
    "count_nb_score = metrics.accuracy_score(y_test, count_nb_pred)\n",
    "\n",
    "print('NaiveBayes Tfidf Score: ', tfidf_nb_score)\n",
    "print('NaiveBayes Count Score: ', count_nb_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Model with a Confusion Matrix\n",
    "\n",
    "I observed that the TF-IDF model outperformed the count-based approach. From what I learned in the NLP fundamentals course, this likely stems from TF-IDF's ability to emphasize unique tokens that might be key identifiers for each tweeter.\n",
    "\n",
    "To comprehensively assess the model, I relied not just on accuracy scores but on the confusion matrix. This matrix highlighted the number of correct and incorrect classifications for each class, using metrics like True Positives, False Positives, False Negatives, and True Negatives. \n",
    "\n",
    "This detailed view helped me understand the modelâ€™s performance, particularly how often Trump was misclassified as Trudeau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from datasets.trump_trudeau.helper_functions import plot_confusion_matrix\n",
    "\n",
    "# Calculate the confusion matrices for the tfidf_nb model and count_nb models\n",
    "tfidf_nb_cm = metrics.confusion_matrix(y_test, tfidf_nb_pred, labels=['Donald J. Trump', 'Justin Trudeau']) \n",
    "count_nb_cm = metrics.confusion_matrix(y_test, count_nb_pred, labels=['Donald J. Trump', 'Justin Trudeau'])\n",
    "\n",
    "# Plot the tfidf_nb_cm confusion matrix\n",
    "plot_confusion_matrix(tfidf_nb_cm, classes=['Donald J. Trump', 'Justin Trudeau'], title=\"TF-IDF NB Confusion Matrix\")\n",
    "\n",
    "# Plot the count_nb_cm confusion matrix without overwriting the first plot \n",
    "plot_confusion_matrix(count_nb_cm, classes=['Donald J. Trump', 'Justin Trudeau'], title=\"Count NB Confusion Matrix\", figure=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting with Linear SVC\n",
    "\n",
    "After evaluating the Bayesian model, which showed minimal difference (only one prediction difference) in predictions between TF-IDF and count vectorizers, I noticed some misclassifications particularly where Trump was predicted for Trudeau's tweets. This prompted an exploration into the problematic tokens to refine the model further.\n",
    "\n",
    "Motivated to explore alternative methods, I decided to try LinearSVC, known for its effectiveness in text classification. I was keen to see if applying it with TF-IDF vectors would improve the accuracy further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a LinearSVC model\n",
    "tfidf_svc = LinearSVC()\n",
    "\n",
    "tfidf_svc.fit(tfidf_train, y_train)\n",
    "\n",
    "# Run predict on your tfidf test data to get your predictions\n",
    "tfidf_svc_pred = tfidf_svc.predict(tfidf_test)\n",
    "\n",
    "# Calculate your accuracy using the metrics module\n",
    "tfidf_svc_score = metrics.accuracy_score(y_test, tfidf_svc_pred)\n",
    "\n",
    "print(\"LinearSVC Score:   %0.3f\" % tfidf_svc_score)\n",
    "\n",
    "# Calculate the confusion matrices for the tfidf_svc model\n",
    "svc_cm = metrics.confusion_matrix(y_test, tfidf_svc_pred, labels=['Donald J. Trump', 'Justin Trudeau'])\n",
    "\n",
    "# Plot the confusion matrix using the plot_confusion_matrix function\n",
    "plot_confusion_matrix(svc_cm, classes=['Donald J. Trump', 'Justin Trudeau'], title=\"TF-IDF LinearSVC Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introspecting the best-performing model\n",
    "\n",
    "I discovered that the LinearSVC model performed even better than the Multinomial Bayesian one, which was a significant achievement. \n",
    "\n",
    "Analyzing the confusion matrix, there were still instances where Trudeau's tweets were misclassified as Trump's, but the False Positive rate had improved compared to the previous model. \n",
    "\n",
    "There is room for further refinement, which I could potentially achieve by enhancing all the previous models through parameter optimization and implementing more effective preprocessing strategies for the tweets.\n",
    "\n",
    "I wanted to find out what the model had learned, so I used the LinearSVC Classifier, which categorized tweets into two classes: Trump and Trudeau. By sorting the features (tokens) by their weight, I was able to identify the most significant tokens for both Trump and Trudeau. \n",
    "\n",
    "This exploration aimed to uncover whether the model had learned distinguishing characteristics that were genuinely useful for differentiating between these two prominent figures. \n",
    "\n",
    "What I found raised intriguing questions about the most 'Trump-like' or 'Trudeau-like' words and whether the model had been able to effectively captured the eessential distinctions between these two politicians and their Tweet vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.trump_trudeau.helper_functions import plot_and_return_top_features\n",
    "\n",
    "# Import pprint from pprint\n",
    "from pprint import pprint\n",
    "\n",
    "# Get the top features using the plot_and_return_top_features function and your top model and tfidf vectorizer\n",
    "top_features = plot_and_return_top_features(tfidf_svc, tfidf_vectorizer)\n",
    "\n",
    "# pprint the top features\n",
    "pprint(top_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweet Impersonations\n",
    "\n",
    "My model recognized Trudeau's tendency to tweet in French. I challenged myself to write a tweet that could deceive the model into thinking it was authored by either Trump or Trudeau. With further tinkering and collation of words that our model associates with each politician, it could possible to craft tweets that would be misclassified. \n",
    "\n",
    "For those proficient in French, I suggest trying a Trudeau impersonation in French. \n",
    "Interestingly, while removing both English and French stop words might streamline preprocessing, it could lower the model's accuracy since Trudeau is the sole French-speaker in our dataset. \n",
    "\n",
    "This observation suggests to me, however, that expanding the dataset to include more French-speaking politicians would probably warrant this preprocessing step in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_tweet = \"America is great!\"\n",
    "trudeau_tweet = \"Canada les\"\n",
    "\n",
    "trump_tweet_vectorized = tfidf_vectorizer.transform([trump_tweet])\n",
    "trudeau_tweet_vectorized = tfidf_vectorizer.transform([trudeau_tweet])\n",
    "\n",
    "trump_tweet_pred = tfidf_svc.predict(trump_tweet_vectorized)\n",
    "trudeau_tweet_pred = tfidf_svc.predict(trudeau_tweet_vectorized)\n",
    "\n",
    "print(\"Predicted Trump tweet\", trump_tweet_pred)\n",
    "print(\"Predicted Trudeau tweet\", trudeau_tweet_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results and Conclusion\n",
    "\n",
    "After thorough experimentation and evaluation, I identified the most effective classification model for distinguishing between tweets from Donald Trump and Justin Trudeau - LinearSVC with TF-IDF vectorizer.\n",
    "\n",
    "The project provided valuable insights into the challenges of tweet classification and demonstrated the feasibility of building accurate classifiers for social media text analysis.\n",
    "\n",
    "Future work on this dataset could involve:\n",
    "- add extra preprocessing to my current workflow, in order to observe how these modifications affect the performance of the classifiers.\n",
    "- firstly, removing URLs is essential as they do not contribute meaningful information to text classification.\n",
    "- secondly, since Trudeau tweets occasionally in French, removing French stop words might reduce noise in the data.\n",
    "- use GridSearchCV to improve both my Bayesian and LinearSVC models by finding the optimal parameters\n",
    "- introspect my Bayesian model to determine what words are used more often by Trump or Trudeau\n",
    "- use tweepy to allow me to access and retrieve more recent tweets to your dataset and retrain to ensure that the classifiers remain effective and relevant\n",
    "- continue writing impersonation tweets as a fun application of the project but also a practical test of the classifier's effectiveness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
